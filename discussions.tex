
The goal of this work is to understand the performance of the LOFAR Direction Independent Pipeline as processing parameters are changed. We modify several parameters and compare the wall clock time taken to process the data. Finally, we study data from queuing jobs and downloading data in order to fully model infrastructure overheads. We compare our model with past runs of the software and discuss the results and implications. We discuss the utility of this model for current and upcoming LOFAR projects. Finally, we note the effectiveness of this modelling technique for understanding the performance of large-scale processing of astronomical data.

\subsection{Software Performance}

We performed several tests to determine the scalability of LOFAR processing with respect to several parameters. We outline our findings below as well as their implications for processing in the context of the LOFAR surveys and other LOFAR projects. 

\subsubsection{Data Size}
We tested broadband LOFAR data ranging in size by a factor of 64, and discover that all our processing steps scale linearly in time with respect of the input data size. We learn that for input data above 16GB, the slope of our scaling relation is higher than for the smaller data sets. The linear scaling of our processing suggests that projects interested in processing massive LOFAR data sets can scale well in terms of processing time. 

As the calibration step concatenates 10 input Subbands, data larger than 16GB shows a higher slope (Figure \ref{fig:ch6_gsmcalsolve_size}), meaning they take longer to process than smaller data. This can be attributed to the large memory requirement for data larger than 160GB which are is likely due to the additional memory requirements of the minimization algorithm.  Splitting the performance model in two also helps make a more accurate processing time prediction as fitting a single linear model would have a large negative y-intercept, predicting negative processing times for data smaller than 2GB. 

Our analysis shows the following results. Overall, the slowest step was the {\fontfamily{qcr}\selectfont gsmcal\_solve} step, and its run time scales more strongly with data size than the other steps (equation \ref{eq:ch6_gsmcalsolve} has the steepest slope). This suggests that as data sizes increase, {\fontfamily{qcr}\selectfont gsmcal\_solve} will increasingly dominate the processing time over the other steps (As seen in Figures \ref{fig:ch6_predict_ateam} and \ref{fig:ch6_gsmcalsolve_size}). This effect is especially prominent for data larger than 16GB (160GB when 10 Subbands are concatenated). As such, it is recommended to avoid calibration of data larger than 160GB. This limitation suggests that science requiring operations on non-averaged data sets, such as long-baseline imaging, will require significant computational requirements for high-fidelity images.

\subsubsection{Calibration Model Size}
We discover that the calibration time scales linearly in as a function of the length of the calibration model, however as a power law with respect to the model's cutoff sensitivity. This is because of the (expected) power law relation between the number of sources and cutoff sensitivity, seen in Figure \ref{fig:ch6_skymodel_size}. We can use this discovery to accelerate the processing time by increasing the flux cutoff to the LOFAR direction independent calibration to 0.5 Jy. Doing so will execute the calibration step in 60\% of the time, saving 83 CPU-h per run. Over the remaining 2000 \texttt{prefactor} runs left in the LOTSS project, this change in sensitivity will save more than 167k CPU-hours. 

Figures \ref{fig:ch6_skymodel_images} show a data set calibrated with sky models with cutoff sensitivities listed in Table \ref{table:skymodel_RMS}, and figures  \ref{fig:ch6_skymodel_rcalib_004_03} and \ref{fig:ch6_skymodel_rcalib_08_15} show the calibration solutions obtained by calibrating with sky models of cutoff ranging from 0.05 Jy to 1.5 Jy. These results suggest that performing gain calibration with less complex, and thus smaller, calibration models will not degrade image and solution quality while taking less than 20\% of processing time. Table \ref{table:skymodel_RMS} also confirms this result. 

While this result is encouraging, there are caveats suggesting future study is required. The results we present are for a single observation and has not been processed through the Direction Dependent Calibration pipeline. This pipeline produces final images used for scientific research. Future work will need to confirm that smaller calibration models used in the \texttt{prefactor} pipeline do not degrade the quality of these final images. Nevertheless, if this result holds, the calibration model threshold can be decreased for a wide range of LOFAR projects, significantly saving processing time and computing resources. 

\subsubsection{Comparison with production runs}

When comparing our model's prediction with real processing runs over the past six months, we note that there are considerable overheads when running on a shared infrastructure vs. when processing data on an isolated (Figure \ref{fig:ch6_prod_gsmcal_times}). The overhead in processing is roughly a factor of two-three from our model. This discrepancy suggests that a model for {\fontfamily{qcr}\selectfont gsmcal\_solve} needs to be built using data when running on a shared environment, to better predict processing time. 

\subsection{Infrastructure Performance}
We tested downloading and extracting LOFAR data of various sizes. Both downloading and extracting are shown to be linear in time with respect to the data size for data up to 32 GB. Beyond those sizes, there is more scatter in data extraction due to high file-system load. This is because load on the worker node's file-system can be unpredictable and can affect the data extraction times negatively. Nevertheless, when comparing our extraction tests and processing for the past six months, the predictions by our models (Figure \ref{fig:ch6_dl_hist}) correspond to the production runs (Figures \ref{fig:ch6_prod_dl_10} and \ref{fig:ch6_prod_dl_64}).  


Part of the LOFAR SKSP processing is done on shared infrastructure, which requires requesting processing time ahead of time for each grant period. Being able to predict the amount of resources required to process data each grant period is required to make a reliable estimate on what resources to request. These results can be used by other projects sharing the SURFsara \texttt{GINA} cluster to predict their processing time before submitting jobs. 

\section{Applications and Conclusions}

Our performance model shows that it is possible to predict the processing time and computational resources used by a complex astronomical pipeline. Our results suggest that LOFAR LoTSS processing can be further optimised without sacrificing the quality of the final product. Additionally, our results are transferable to other scientific pipelines that process LOFAR data with different parameters.  Any pipeline that performs gain calibration or application of calibration solutions will benefit from these results. 

In order to provide LOFAR processing as a service to scientific users, we need to estimate the processing time for each request. We need this estimation in order to determine whether the user has sufficient resources left in their resource pool. Knowing the performance of the software pipelines as a function of the input parameters will help predict the run time for each request and the resources consumed. Knowing this will make it possible to notify users how long the request will take and how much of their quota will be depleted. It is important to note that while this model is specific to LOFAR processing, the method we detail can be used by other scientific teams in order to predict the computational requirements for their pipelines. Doing this is necessary if large scale scientific processing is to be offered as a service to the wider community.

Finally, a performance model of the LOFAR software will help make predictions on the time and resources needed to process data for other telescopes such as the Square Kilometer Array (SKA). Once operational, the SKA is expected to produce Exabytes of data per year. Processing this data efficiently requires understanding the scalability of the software performance to facilitate scheduling and resource allotment. Overall, we show that our method helps guide algorithm development in radio astronomy, can be used to predict resource usage by complex pipelines and will be promising in optimising data processing by future telescopes. 
